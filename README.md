# ğŸš€ poc-01-llm-inference-platform

## ğŸ“Œ Opis projektu

Proof of Concept platformy do inferencji LLM uruchomionej na Kubernetes,
zbudowanej w oparciu o:

-   ğŸ§  **Ollama (llm-runtime)** -- runtime do obsÅ‚ugi modeli LLM
-   âš¡ **FastAPI (llm-api)** -- warstwa API z limiterem wspÃ³Å‚bieÅ¼noÅ›ci
-   ğŸ“Š **Prometheus** -- zbieranie metryk
-   ğŸ“ˆ **Grafana** -- wizualizacja i dashboardy
-   ğŸ“¦ **Helm Charts** -- deklaratywne deploymenty
-   ğŸ” GotowoÅ›Ä‡ pod przyszÅ‚e **ArgoCD (GitOps)**

Projekt zostaÅ‚ zaprojektowany tak, aby byÅ‚: - Reprodukowalny - PrzenoÅ›ny
(portability-first) - Monitoring-first - Gotowy pod rozwÃ³j produkcyjny

------------------------------------------------------------------------

# ğŸ— Architektura

``` mermaid
flowchart LR
    User -->|HTTP| LLM_API
    LLM_API -->|HTTP| LLM_RUNTIME
    LLM_API -->|/metrics| Prometheus
    Prometheus --> Grafana
```

------------------------------------------------------------------------

# âš¡ Quick Start (5 minut)

### 1ï¸âƒ£ Uruchom klaster (np. Colima + Minikube)

Upewnij siÄ™, Å¼e masz min. 4--5GB RAM dla klastra.

### 2ï¸âƒ£ Uruchom bootstrap

``` bash
./scripts/bootstrap.sh
```

Skrypt: - Tworzy namespace `llm` - Tworzy namespace `monitoring` -
Deployuje llm-runtime - Deployuje llm-api - Deployuje Prometheus stack -
Tworzy ServiceMonitor

### 3ï¸âƒ£ Port-forward

``` bash
kubectl port-forward svc/llm-api -n llm 8000:8000
kubectl port-forward svc/prometheus-grafana -n monitoring 3000:80
```

### 4ï¸âƒ£ Test requestu

``` bash
curl -X POST "http://localhost:8000/generate?prompt=2%2B2%3D%3F"
```

------------------------------------------------------------------------

# ğŸ“Š Monitoring i metryki

API eksportuje:

-   `llm_requests_total`
-   `llm_inference_seconds_bucket`
-   `llm_inference_seconds_sum`
-   `llm_inference_seconds_count`

### ğŸ“ˆ Dashboard Grafana

Dashboard zawiera:

-   Requests per second (RPS)
-   Average inference duration
-   P95 latency
-   Error rate

PromQL przykÅ‚ady:

**RPS**

    rate(llm_requests_total[1m])

**Åšredni czas inferencji**

    rate(llm_inference_seconds_sum[1m]) 
    / rate(llm_inference_seconds_count[1m])

**P95**

    histogram_quantile(0.95, rate(llm_inference_seconds_bucket[1m]))

------------------------------------------------------------------------

# ğŸ§  llm-api -- cechy

-   Asynchroniczne requesty (httpx.AsyncClient)
-   Semaphore limiter
-   Dynamiczne wykrywanie aktywnego modelu
-   Histogram metryk
-   Logowanie z request_id

------------------------------------------------------------------------

# ğŸ“ Struktura repozytorium

    charts/
      llm-runtime/
      llm-api/

    platform/
      monitoring/
        prometheus-stack/
        servicemonitor.yaml

    scripts/
      bootstrap.sh

    api/
      main.py

------------------------------------------------------------------------

# ğŸ¯ Cele projektu

-   Demonstracja LLM inference platformy
-   Monitoring-first mindset
-   GotowoÅ›Ä‡ pod GitOps
-   Fundament pod skalowanie (HPA, autoscaling, multi-model)

------------------------------------------------------------------------

# ğŸ”® Kolejne kroki

-   Autoscaling llm-api
-   Resource limits tuning
-   Load testing
-   ArgoCD deployment
-   Alerty w Prometheus
-   Tracing (OpenTelemetry)

------------------------------------------------------------------------

# ğŸ‘¨â€ğŸ’» Autor

Maciej Åuszcz\
TOMA Software\
DevSecOps \| Cloud Native \| AI Platform Engineering

------------------------------------------------------------------------

# ğŸ Status

âœ”ï¸ LLM dziaÅ‚a\
âœ”ï¸ Monitoring dziaÅ‚a\
âœ”ï¸ Dashboard dziaÅ‚a\
âœ”ï¸ Bootstrap automatyzuje klaster

Projekt rozwijany dalej ğŸš€
