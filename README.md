# ğŸš€ PoC #1 --- Lokalna Platforma Inference LLM (Kubernetes + Ollama)

## ğŸ“Œ Opis projektu

Projekt demonstracyjny przedstawiajÄ…cy lokalnÄ… platformÄ™ inference LLM
uruchomionÄ… w Å›rodowisku Kubernetes z wykorzystaniem:

-   ğŸ§  **Ollama** jako runtime modelu LLM
-   âš™ï¸ **FastAPI** jako warstwy API
-   ğŸ“¦ **Helm** do zarzÄ…dzania cyklem Å¼ycia aplikacji
-   â˜¸ï¸ **Minikube** jako lokalny klaster Kubernetes

System realizuje peÅ‚ny przepÅ‚yw inference:

Klient â†’ FastAPI â†’ Ollama â†’ Model â†’ OdpowiedÅº

------------------------------------------------------------------------

## ğŸ— Architektura

    +--------------------+
    |      Klient        |
    |      (curl)        |
    +---------+----------+
              |
              v
    +--------------------+
    |   FastAPI (API)    |
    |  llm-llm-platform  |
    +---------+----------+
              |
              | http://ollama:11434
              v
    +--------------------+
    |      Ollama        |
    |   (Runtime LLM)    |
    +--------------------+

### Wykorzystane zasoby Kubernetes

-   Deployment -- warstwa API (FastAPI)
-   Deployment -- runtime LLM (Ollama)
-   Service (ClusterIP) -- komunikacja wewnÄ™trzna
-   Helm -- zarzÄ…dzanie release
-   Liveness & Readiness Probes
-   Resource Requests & Limits

------------------------------------------------------------------------

## ğŸ›  Wymagania

-   Docker / Colima
-   Minikube
-   kubectl
-   Helm
-   Python 3.11+

------------------------------------------------------------------------

## â–¶ï¸ Uruchomienie

### 1ï¸âƒ£ Start klastra

``` bash
minikube start --driver=docker --memory=2899 --cpus=2
```

------------------------------------------------------------------------

### 2ï¸âƒ£ Budowa obrazu API wewnÄ…trz klastra

``` bash
minikube image build -t llm-api:0.1 ./api
```

------------------------------------------------------------------------

### 3ï¸âƒ£ Deployment przez Helm

``` bash
helm upgrade --install llm ./llm-platform
```

------------------------------------------------------------------------

### 4ï¸âƒ£ Pobranie modelu LLM

``` bash
kubectl exec -it deploy/ollama -- ollama pull tinyllama
```

------------------------------------------------------------------------

### 5ï¸âƒ£ Test inference

``` bash
kubectl port-forward deploy/llm-llm-platform 8000:8000
```

W drugim terminalu:

``` bash
curl -X POST "localhost:8000/generate?prompt=Hello"
```

------------------------------------------------------------------------

## ğŸ”„ Aktualizacja aplikacji

Po zmianie kodu API:

``` bash
minikube image build -t llm-api:<nowy-tag> ./api
helm upgrade llm ./llm-platform --set image.tag=<nowy-tag>
```

Rollout wykona siÄ™ automatycznie -- bez potrzeby rÄ™cznego restartu.

------------------------------------------------------------------------

## ğŸ§  Co demonstruje ten projekt

-   Uruchomienie LLM w Kubernetes lokalnie\
-   KomunikacjÄ™ service-to-service przez DNS klastra\
-   ZarzÄ…dzanie cyklem Å¼ycia aplikacji przez Helm\
-   Debugowanie ReplicaSet, ServiceAccount, Probes i ImagePull\
-   KontrolÄ™ zasobÃ³w (requests / limits)\
-   Oddzielenie warstwy API od runtime modelu

------------------------------------------------------------------------

## ğŸš§ MoÅ¼liwe dalsze kroki

-   PersistentVolume dla przechowywania modeli
-   InitContainer do automatycznego pobierania modelu
-   Integracja z ArgoCD (GitOps)
-   Monitoring (Prometheus)
-   Autoskalowanie (HPA)

------------------------------------------------------------------------

## ğŸ¯ Cel PoC

Weryfikacja moÅ¼liwoÅ›ci uruchomienia lekkiej, samowystarczalnej platformy
inference LLM w peÅ‚ni w Å›rodowisku Kubernetes, bez zaleÅ¼noÅ›ci od usÅ‚ug
chmurowych.

------------------------------------------------------------------------

## ğŸ‘¤ Autor

Maciej Åuszcz\
TOMA Software
