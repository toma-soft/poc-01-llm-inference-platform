# ğŸš€ LLM Inference Platform (PoC)

## ğŸ“Œ Opis projektu

Projekt **poc-01-llm-inference-platform** to praktyczne laboratorium
DevOps + AI, ktÃ³rego celem jest zbudowanie mini platformy inferencyjnej
dla modeli LLM z wykorzystaniem Kubernetes, Helm, Prometheus, Grafana
oraz autoskalowania HPA.

Projekt ewoluowaÅ‚ z prostego PoC do **mini platformy produkcyjnej klasy
"real-world ready"**, z monitoringiem, metrykami i autoskalowaniem CPU.

------------------------------------------------------------------------

## ğŸ¯ Cele projektu

-   Budowa API dla inferencji LLM (FastAPI)
-   Uruchomienie runtime (Ollama) w Kubernetes
-   Monitoring metryk aplikacyjnych i infrastrukturalnych
-   Integracja z Prometheus + Grafana
-   Implementacja HPA (Horizontal Pod Autoscaler)
-   Przygotowanie chartÃ³w Helm pod przyszÅ‚e ArgoCD
-   Automatyczny bootstrap klastra

------------------------------------------------------------------------

## ğŸ— Architektura

UÅ¼ytkownik â†’ LLM API (FastAPI) â†’ LLM Runtime (Ollama) â†’ Model (np.
gemma)

Monitoring: - Prometheus (kube-prometheus-stack) - Grafana (custom
dashboard) - metrics-server (CPU metrics dla HPA)

Autoskalowanie: - HPA dla `llm-api` - HPA dla `llm-runtime` (CPU-based +
behavior)

------------------------------------------------------------------------

## ğŸ“‚ Struktura repozytorium

    api/
      Dockerfile
      main.py
      requirements.txt

    charts/
      llm-api/
      llm-runtime/

    platform/monitoring/
      dashboards/
      prometheus-stack/

    scripts/
      bootstrap.sh

    README.md

------------------------------------------------------------------------

## ğŸ“Š Monitoring metryk

Platforma posiada:

-   Prometheus (kube-prometheus-stack)
-   Grafana
-   Custom dashboard â€LLM Platform Dashboard"

Dashboard zawiera:

-   Requests per second (RPS)
-   Average inference latency
-   P95 inference duration
-   Errors per second

Metryki aplikacyjne:

-   `llm_requests_total`
-   `llm_inference_seconds_bucket`
-   `llm_inference_seconds_sum`
-   `llm_inference_seconds_count`

Dashboard jest utrwalony jako ConfigMap (nie jest juÅ¼ ulotny).

------------------------------------------------------------------------

## ğŸ”Œ LLM API

Technologie: - FastAPI - httpx (async) - Prometheus client - Semaphore
(limit rÃ³wnolegÅ‚ych requestÃ³w)

FunkcjonalnoÅ›ci: - `/generate` - `/health` - `/metrics`

Metryki: - liczba requestÃ³w - histogram czasu inferencji

Autoskalowanie: - HPA CPU-based - target CPU: 60% - minReplicas: 1 -
maxReplicas: 3

------------------------------------------------------------------------

## ğŸ§  LLM Runtime

Runtime oparty o **Ollama**.

FunkcjonalnoÅ›ci: - uruchamianie modelu (np. gemma) - obsÅ‚uga wielu
requestÃ³w - autoskalowanie HPA

HPA dla runtime:

-   target CPU: 70%
-   minReplicas: 1
-   maxReplicas: 2
-   behavior:
    -   szybki scale-up
    -   opÃ³Åºniony scale-down (stabilization window)

Runtime skaluje siÄ™ pod obciÄ…Å¼eniem (CPU \~500% â†’ 2 repliki), a po
zakoÅ„czeniu obciÄ…Å¼enia wraca do 1 pod.

------------------------------------------------------------------------

## âš™ï¸ Bootstrap klastra

Automatyczny skrypt:

    ./scripts/bootstrap.sh

Wykonuje:

-   tworzenie namespace
-   deployment llm-runtime
-   deployment llm-api
-   deployment Prometheus stack
-   aplikacjÄ™ ServiceMonitor

------------------------------------------------------------------------

## ğŸ“ˆ Co juÅ¼ dziaÅ‚a

-   API z metrykami
-   Runtime z autoskalowaniem
-   Monitoring z dashboardem
-   CPU-based HPA
-   behavior w HPA
-   Automatyczny bootstrap
-   Helm charts gotowe pod GitOps

------------------------------------------------------------------------

## ğŸ”® Kolejne kroki

-   Ingress + cert-manager
-   Custom metrics HPA (RPS-based)
-   Load testing (k6)
-   ArgoCD
-   Resource limits tuning
-   Production-grade values.yaml

------------------------------------------------------------------------

## ğŸ Status

Projekt edukacyjny + DevOps lab. Mini platforma inferencyjna gotowa do
dalszego rozwoju.

