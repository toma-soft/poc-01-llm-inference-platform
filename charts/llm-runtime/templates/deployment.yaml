apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-runtime
  template:
    metadata:
      labels:
        app: llm-runtime
    spec:
      initContainers:
        - name: model-pull
          image: ollama/ollama
          command:
            - sh
            - -c
            - |
              echo "Starting temporary Ollama server..."
              ollama serve &
              sleep 5
              echo "Pulling model {{ .Values.model.name }}"
              ollama pull {{ .Values.model.name }} || true
          volumeMounts:
            - name: ollama-storage
              mountPath: /root/.ollama
      containers:
        - name: ollama
          image: ollama/ollama
          readinessProbe:
            httpGet:
                path: /
                port: 11434
            initialDelaySeconds: 5
            periodSeconds: 10
          livenessProbe:
            httpGet:
                path: /
                port: 11434
            initialDelaySeconds: 5
            periodSeconds: 10
          ports:
            - containerPort: 11434
          env:
            - name: RUNTIME_HOST
              value: "{{ .Values.runtime.host }}"
            - name: RUNTIME_PORT
              value: "{{ .Values.runtime.port }}"
            - name: TIMEOUT_SECONDS
              value: "{{ .Values.timeoutSeconds }}"
            - name: MAX_CONCURRENT_REQUESTS
              value: "{{ .Values.maxConcurrentRequests }}"
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
            limits:
              cpu: 500m
              memory: 1500Mi
          volumeMounts:
            - name: ollama-storage
              mountPath: /root/.ollama    
      volumes:
        - name: ollama-storage
          persistentVolumeClaim:
            claimName: ollama-runtime-pvc
